<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2025-09-27 Sat 21:01 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Access state-of-the-art Large Language Models &amp; APIs</title>
<meta name="author" content="Zi Liang" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="/css/styles.css" /> <link rel="stylesheet" type="text/css" href="/css/htmlize.css" /> <script src="/scripts/script.js"></script> <script src="/scripts/toc.js"></script>
</head>
<body>
<div id="preamble" class="status">
<nav class="nav"> <a href="/index.html" class="button">Home</a> <a href="/sitemap.html" class="button">Sitemaps</a> </nav> <hr>
</div>
<div id="content" class="content">
<h1 class="title">Access state-of-the-art Large Language Models &amp; APIs</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7c71f80">1. Overview</a></li>
<li><a href="#orge65e2b0">2. APIs</a>
<ul>
<li><a href="#org2b15810">2.1. GPT-3.5-turbo (ChatGPT) &amp; GPT-4</a></li>
<li><a href="#orgd931a47">2.2. Bard</a></li>
<li><a href="#org2b369ab">2.3. Claude</a>
<ul>
<li><a href="#org0e05010">2.3.1. Step 1: set on slack, to obtain a access token to slack as well as a APP ID of your claude chatbot</a></li>
<li><a href="#orge82b09e">2.3.2. Step 2: use the API of claude</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0d8fcbf">3. LLMs</a>
<ul>
<li><a href="#orgbdfc4ce">3.1. Llama &amp; Llama2</a></li>
<li><a href="#org54e2faf">3.2. By candle, to obtain llama &amp; llama2 &amp; falcon</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
It is common to evalute existing LLMs under certain benchmarks. This article is a summerization of some of the existing state-of-the-art language models.
</p>
<div id="outline-container-org7c71f80" class="outline-2">
<h2 id="org7c71f80"><span class="section-number-2">1.</span> Overview</h2>
<div class="outline-text-2" id="text-1">
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Name</th>
<th scope="col" class="org-left">is_official</th>
<th scope="col" class="org-left">is_API</th>
<th scope="col" class="org-left">Device</th>
<th scope="col" class="org-left">Proxy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">openai</td>
<td class="org-left">Y</td>
<td class="org-left">Y</td>
<td class="org-left">both</td>
<td class="org-left">Y</td>
</tr>

<tr>
<td class="org-left">Bard</td>
<td class="org-left">N</td>
<td class="org-left">Y</td>
<td class="org-left">both</td>
<td class="org-left">Y</td>
</tr>

<tr>
<td class="org-left">Claude</td>
<td class="org-left">N</td>
<td class="org-left">Y</td>
<td class="org-left">both</td>
<td class="org-left">N</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orge65e2b0" class="outline-2">
<h2 id="orge65e2b0"><span class="section-number-2">2.</span> APIs</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org2b15810" class="outline-3">
<h3 id="org2b15810"><span class="section-number-3">2.1.</span> GPT-3.5-turbo (ChatGPT) &amp; GPT-4</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>OS: Unbuntu 18.04, also works in Linux mint, Windows, and other environments</li>
<li>Device: both</li>
<li>link: <a href="https://github.com/openai/openai-python">https://github.com/openai/openai-python</a></li>
<li>proxy: yes</li>
</ul>


<p>
Usage to evalute the politness:
</p>


<div class="org-src-container">
<pre class="src src-python">
<span style="color: #565575;"># </span><span style="color: #96a0aa;">pip install openai
</span><span style="color: #ff8080; font-weight: bold;">import</span> openai
<span style="color: #ff8080; font-weight: bold;">import</span> os


<span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">reference_one_sample</span><span style="color: #c66; font-weight: bold;">(</span>text<span style="color: #c66; font-weight: bold;">)</span>:
    openai.<span style="color: #ffe9aa;">api_key</span>=os.getenv<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"OPENAI_KEY"</span><span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #ffe9aa;">past_msg_ls</span>=<span style="color: #c66; font-weight: bold;">[]</span>
    past_msg_ls.append<span style="color: #c66; font-weight: bold;">(</span><span style="color: #6c6; font-weight: bold;">{</span><span style="color: #ffe9aa; font-style: italic;">"role"</span>:<span style="color: #ffe9aa; font-style: italic;">"user"</span>,
                        <span style="color: #ffe9aa; font-style: italic;">"content"</span>:text<span style="color: #6c6; font-weight: bold;">}</span><span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #ffe9aa;">res</span>=openai.ChatCompletion.create<span style="color: #c66; font-weight: bold;">(</span>model=<span style="color: #ffe9aa; font-style: italic;">"gpt-3.5-turbo"</span>,
                                        messages=past_msg_ls,
                                        <span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #ffe9aa;">resp</span>=res<span style="color: #c66; font-weight: bold;">[</span><span style="color: #ffe9aa; font-style: italic;">"choices"</span><span style="color: #c66; font-weight: bold;">][</span>0<span style="color: #c66; font-weight: bold;">][</span><span style="color: #ffe9aa; font-style: italic;">'message'</span><span style="color: #c66; font-weight: bold;">][</span><span style="color: #ffe9aa; font-style: italic;">'content'</span><span style="color: #c66; font-weight: bold;">]</span>
    <span style="color: #ff8080; font-weight: bold;">return</span> resp


<span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">main</span><span style="color: #c66; font-weight: bold;">()</span>:
    <span style="color: #ffe9aa;">res</span>=reference_one_sample<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"fuck your mother. your son of bitch."</span><span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #c991e1;">print</span><span style="color: #c66; font-weight: bold;">(</span>res<span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #ff8080; font-weight: bold;">return</span> 0


</pre>
</div>
</div>
</div>
<div id="outline-container-orgd931a47" class="outline-3">
<h3 id="orgd931a47"><span class="section-number-3">2.2.</span> Bard</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>OS: Unbuntu 18.04, also works in Linux mint, Windows, and other environments</li>
<li>Device: both</li>
<li>link: <a href="https://github.com/dsdanielpark/Bard-API">https://github.com/dsdanielpark/Bard-API</a></li>
<li>proxy: yes</li>
<li>is official: NO</li>
<li>original app link: <a href="https://bard.google.com/">https://bard.google.com/</a></li>
</ul>


<p>
Usages:
</p>


<div class="org-src-container">
<pre class="src src-python">
<span style="color: #565575;"># </span><span style="color: #96a0aa;">pip install bardapi 
</span><span style="color: #ff8080; font-weight: bold;">from</span> bardapi <span style="color: #ff8080; font-weight: bold;">import</span> Bard
<span style="color: #ff8080; font-weight: bold;">import</span> os

<span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">inference_one_sample</span><span style="color: #c66; font-weight: bold;">(</span>text<span style="color: #c66; font-weight: bold;">)</span>:
    <span style="color: #ffe9aa;">token_bard</span>=os.getenv<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"BARD_KEY"</span><span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #c991e1;">print</span><span style="color: #c66; font-weight: bold;">(</span>token_bard<span style="color: #c66; font-weight: bold;">)</span>

    <span style="color: #ffe9aa;">proxies</span> = <span style="color: #c66; font-weight: bold;">{</span>
    <span style="color: #ffe9aa; font-style: italic;">'http'</span>: <span style="color: #ffe9aa; font-style: italic;">'http://username:pswd@ip:port'</span>,
    <span style="color: #ffe9aa; font-style: italic;">'https'</span>: <span style="color: #ffe9aa; font-style: italic;">'http://uname:pswd@ip:port'</span>
    <span style="color: #c66; font-weight: bold;">}</span>
    <span style="color: #ffe9aa;">session</span> = requests.Session<span style="color: #c66; font-weight: bold;">()</span>
    session.<span style="color: #ffe9aa;">headers</span> = <span style="color: #c66; font-weight: bold;">{</span>
            <span style="color: #ffe9aa; font-style: italic;">"Host"</span>: <span style="color: #ffe9aa; font-style: italic;">"bard.google.com"</span>,
            <span style="color: #ffe9aa; font-style: italic;">"X-Same-Domain"</span>: <span style="color: #ffe9aa; font-style: italic;">"1"</span>,
            <span style="color: #ffe9aa; font-style: italic;">"User-Agent"</span>: <span style="color: #ffe9aa; font-style: italic;">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"</span>,
            <span style="color: #ffe9aa; font-style: italic;">"Content-Type"</span>: <span style="color: #ffe9aa; font-style: italic;">"application/x-www-form-urlencoded;charset=UTF-8"</span>,
            <span style="color: #ffe9aa; font-style: italic;">"Origin"</span>: <span style="color: #ffe9aa; font-style: italic;">"http://bard.google.com"</span>,
            <span style="color: #ffe9aa; font-style: italic;">"Referer"</span>: <span style="color: #ffe9aa; font-style: italic;">"http://bard.google.com/"</span>,
        <span style="color: #c66; font-weight: bold;">}</span>
    session.cookies.<span style="color: #c991e1;">set</span><span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"__Secure-1PSID"</span>, os.getenv<span style="color: #6c6; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"BARD_KEY"</span><span style="color: #6c6; font-weight: bold;">)</span><span style="color: #c66; font-weight: bold;">)</span> 

    <span style="color: #ffe9aa;">bard</span> = Bard<span style="color: #c66; font-weight: bold;">(</span>token=token_bard,
                session=session,
                proxies=proxies,
                timeout=30<span style="color: #c66; font-weight: bold;">)</span>

    <span style="color: #ffe9aa;">res</span>=bard.get_answer<span style="color: #c66; font-weight: bold;">(</span>text<span style="color: #c66; font-weight: bold;">)[</span><span style="color: #ffe9aa; font-style: italic;">'content'</span><span style="color: #c66; font-weight: bold;">]</span>
    <span style="color: #c991e1;">print</span><span style="color: #c66; font-weight: bold;">(</span>res<span style="color: #c66; font-weight: bold;">)</span>

<span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">main</span><span style="color: #c66; font-weight: bold;">()</span>:
    inference_one_sample<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"1+1=?"</span><span style="color: #c66; font-weight: bold;">)</span>
</pre>
</div>

<p>
How to obtain the <code>__Secure-1PSID</code> ?
</p>

<ol class="org-ol">
<li>Sign up or log into the website <a href="https://bard.google.com/">https://bard.google.com/</a> in webbrowser.</li>
<li>Open development kits, like <code>Ctrl-Shift-I</code></li>
<li>find this value in cookies, may be stored in your variables.</li>
</ol>

<p>
using time.sleep(60*15) to control the access frequency to one time per 15 minutes.
</p>
</div>
</div>
<div id="outline-container-org2b369ab" class="outline-3">
<h3 id="org2b369ab"><span class="section-number-3">2.3.</span> Claude</h3>
<div class="outline-text-3" id="text-2-3">
<p>
NOTE: for claude I use the slack API to send/receive messages to/from the claude chatbot in slack.
</p>

<ul class="org-ul">
<li>OS: Unbuntu 18.04, also works in Linux mint, Windows, and other environments</li>
<li>Device: both</li>
<li>link: none</li>
<li>proxy: not required</li>
<li>is official: NO</li>
<li>original app link:</li>
<li>docuement of use slack API</li>
</ul>
</div>
<div id="outline-container-org0e05010" class="outline-4">
<h4 id="org0e05010"><span class="section-number-4">2.3.1.</span> Step 1: set on slack, to obtain a access token to slack as well as a APP ID of your claude chatbot</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
lookup this article: <a href="https://mp.weixin.qq.com/s?__biz=Mzg4MjkzMzc1Mg==&amp;mid=2247483961&amp;idx=1&amp;sn=c009f4ea28287daeaa4de17278c8228e&amp;chksm=cf4e68aef839e1b8fe49110341e2a557e0b118fee82d490143656a12c7f85bdd4ef6f65ffd16&amp;token=1094126126&amp;lang=zh_CN#rd">blablabla</a>
</p>
</div>
</div>
<div id="outline-container-orge82b09e" class="outline-4">
<h4 id="orge82b09e"><span class="section-number-4">2.3.2.</span> Step 2: use the API of claude</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
Usages:
</p>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #ff8080; font-weight: bold;">import</span> os
<span style="color: #ff8080; font-weight: bold;">import</span> requests

<span style="color: #565575;"># </span><span style="color: #96a0aa;">pip install slack-sdk
</span><span style="color: #ff8080; font-weight: bold;">from</span> slack_sdk <span style="color: #ff8080; font-weight: bold;">import</span> WebClient
<span style="color: #ff8080; font-weight: bold;">from</span> slack_sdk.errors <span style="color: #ff8080; font-weight: bold;">import</span> SlackApiError

<span style="color: #ff8080; font-weight: bold;">class</span> <span style="color: #91ddff;">SlackClient</span><span style="color: #c66; font-weight: bold;">(</span>WebClient<span style="color: #c66; font-weight: bold;">)</span>:
    <span style="color: #ffe9aa;">CHANNEL_ID</span> = <span style="color: #aaffe4;">None</span> 
    <span style="color: #ffe9aa;">CLAUDE_BOT_ID</span>=os.getenv<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"CLAUDE_BOT_ID"</span><span style="color: #c66; font-weight: bold;">)</span>

    <span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">chat</span><span style="color: #c66; font-weight: bold;">(</span><span style="color: #ff8080; font-weight: bold;">self</span>, text<span style="color: #c66; font-weight: bold;">)</span>:
        <span style="color: #ff8080; font-weight: bold;">if</span> <span style="color: #ff8080; font-weight: bold;">not</span> <span style="color: #ff8080; font-weight: bold;">self</span>.CHANNEL_ID:
            <span style="color: #ff8080; font-weight: bold;">raise</span> <span style="color: #91ddff;">Exception</span><span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"Channel not found."</span><span style="color: #c66; font-weight: bold;">)</span>
        <span style="color: #ffe9aa;">resp</span> = <span style="color: #ff8080; font-weight: bold;">self</span>.chat_postMessage<span style="color: #c66; font-weight: bold;">(</span>channel=<span style="color: #ff8080; font-weight: bold;">self</span>.CHANNEL_ID, text=text<span style="color: #c66; font-weight: bold;">)</span>
        <span style="color: #ff8080; font-weight: bold;">self</span>.<span style="color: #ffe9aa;">LAST_TS</span> = resp<span style="color: #c66; font-weight: bold;">[</span><span style="color: #ffe9aa; font-style: italic;">"ts"</span><span style="color: #c66; font-weight: bold;">]</span>

    <span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">get_reply</span><span style="color: #c66; font-weight: bold;">(</span><span style="color: #ff8080; font-weight: bold;">self</span><span style="color: #c66; font-weight: bold;">)</span>:
        <span style="color: #ff8080; font-weight: bold;">for</span> _ <span style="color: #ff8080; font-weight: bold;">in</span> <span style="color: #c991e1;">range</span><span style="color: #c66; font-weight: bold;">(</span>150<span style="color: #c66; font-weight: bold;">)</span>:
            <span style="color: #ff8080; font-weight: bold;">try</span>:
                <span style="color: #ffe9aa;">resp</span> = <span style="color: #ff8080; font-weight: bold;">self</span>.conversations_history<span style="color: #c66; font-weight: bold;">(</span>channel=<span style="color: #ff8080; font-weight: bold;">self</span>.CHANNEL_ID, oldest=<span style="color: #ff8080; font-weight: bold;">self</span>.LAST_TS, limit=2<span style="color: #c66; font-weight: bold;">)</span>
                <span style="color: #ffe9aa;">msg</span> = <span style="color: #c66; font-weight: bold;">[</span>msg<span style="color: #6c6; font-weight: bold;">[</span><span style="color: #ffe9aa; font-style: italic;">"text"</span><span style="color: #6c6; font-weight: bold;">]</span> <span style="color: #ff8080; font-weight: bold;">for</span> msg <span style="color: #ff8080; font-weight: bold;">in</span> resp<span style="color: #6c6; font-weight: bold;">[</span><span style="color: #ffe9aa; font-style: italic;">"messages"</span><span style="color: #6c6; font-weight: bold;">]</span>\
                       <span style="color: #ff8080; font-weight: bold;">if</span> msg<span style="color: #6c6; font-weight: bold;">[</span><span style="color: #ffe9aa; font-style: italic;">"user"</span><span style="color: #6c6; font-weight: bold;">]</span> == <span style="color: #ff8080; font-weight: bold;">self</span>.CLAUDE_BOT_ID<span style="color: #c66; font-weight: bold;">]</span>
                <span style="color: #ff8080; font-weight: bold;">if</span> msg <span style="color: #ff8080; font-weight: bold;">and</span> <span style="color: #ff8080; font-weight: bold;">not</span> msg<span style="color: #c66; font-weight: bold;">[</span>-1<span style="color: #c66; font-weight: bold;">]</span>.endswith<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"Typing&#8230;_"</span><span style="color: #c66; font-weight: bold;">)</span>:
                    <span style="color: #ff8080; font-weight: bold;">return</span> msg<span style="color: #c66; font-weight: bold;">[</span>-1<span style="color: #c66; font-weight: bold;">]</span>
            <span style="color: #ff8080; font-weight: bold;">except</span> <span style="color: #c66; font-weight: bold;">(</span>SlackApiError, <span style="color: #91ddff;">KeyError</span><span style="color: #c66; font-weight: bold;">)</span> <span style="color: #ff8080; font-weight: bold;">as</span> e:
                <span style="color: #c991e1;">print</span><span style="color: #c66; font-weight: bold;">(</span>f<span style="color: #ffe9aa; font-style: italic;">"Get reply error: </span>{e}<span style="color: #ffe9aa; font-style: italic;">"</span><span style="color: #c66; font-weight: bold;">)</span>
                <span style="color: #ff8080; font-weight: bold;">raise</span> <span style="color: #91ddff;">Exception</span><span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"Get replay timeout"</span><span style="color: #c66; font-weight: bold;">)</span>

    <span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">open_channel</span><span style="color: #c66; font-weight: bold;">(</span><span style="color: #ff8080; font-weight: bold;">self</span><span style="color: #c66; font-weight: bold;">)</span>:
        <span style="color: #ffe9aa;">response</span> = <span style="color: #ff8080; font-weight: bold;">self</span>.conversations_open<span style="color: #c66; font-weight: bold;">(</span>users=<span style="color: #ff8080; font-weight: bold;">self</span>.CLAUDE_BOT_ID<span style="color: #c66; font-weight: bold;">)</span>
        <span style="color: #ff8080; font-weight: bold;">self</span>.<span style="color: #ffe9aa;">CHANNEL_ID</span> = response<span style="color: #c66; font-weight: bold;">[</span><span style="color: #ffe9aa; font-style: italic;">"channel"</span><span style="color: #c66; font-weight: bold;">][</span><span style="color: #ffe9aa; font-style: italic;">"id"</span><span style="color: #c66; font-weight: bold;">]</span>

<span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">forward</span><span style="color: #c66; font-weight: bold;">(</span>text<span style="color: #c66; font-weight: bold;">)</span>:
    <span style="color: #ffe9aa;">client</span> = SlackClient<span style="color: #c66; font-weight: bold;">(</span>token=os.getenv<span style="color: #6c6; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"SLACK_USER_TOKEN"</span><span style="color: #6c6; font-weight: bold;">)</span><span style="color: #c66; font-weight: bold;">)</span> 
    client.open_channel<span style="color: #c66; font-weight: bold;">()</span>
    client.chat<span style="color: #c66; font-weight: bold;">(</span>text<span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #ffe9aa;">reply</span> = client.get_reply<span style="color: #c66; font-weight: bold;">()</span>
    <span style="color: #c991e1;">print</span><span style="color: #c66; font-weight: bold;">(</span>f<span style="color: #ffe9aa; font-style: italic;">"Claude: </span>{reply}<span style="color: #aaffe4;">\n</span><span style="color: #ffe9aa; font-style: italic;">--------------------"</span><span style="color: #c66; font-weight: bold;">)</span>

<span style="color: #565575;">## </span><span style="color: #96a0aa;">running entry
</span><span style="color: #ff8080; font-weight: bold;">if</span> <span style="color: #c991e1;">__name__</span>==<span style="color: #ffe9aa; font-style: italic;">"__main__"</span>:
    <span style="color: #565575;"># </span><span style="color: #96a0aa;">infer_one("hello! nice to meet you!")
</span>    <span style="color: #565575;"># </span><span style="color: #96a0aa;">infer_one("hello! nice to meet you!")
</span>    forward<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"do you know budda?"</span><span style="color: #c66; font-weight: bold;">)</span>
</pre>
</div>

<p>
using time.sleep(60*5) to control the access frequency to one time per 5 minutes.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org0d8fcbf" class="outline-2">
<h2 id="org0d8fcbf"><span class="section-number-2">3.</span> LLMs</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgbdfc4ce" class="outline-3">
<h3 id="orgbdfc4ce"><span class="section-number-3">3.1.</span> Llama &amp; Llama2</h3>
<div class="outline-text-3" id="text-3-1">
<p>
you can use llama.cpp to run different version of LLAMA models.
</p>


<ul class="org-ul">
<li>OS: Linux recommanded</li>
<li>Device:  GPU recommanded</li>
<li>link: <a href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></li>
<li>proxy: not required</li>
<li>is official: NA</li>
</ul>


<p>
First compile this repository in your device:
</p>



<div class="org-src-container">
<pre class="src src-sh"><span style="color: #565575;"># </span><span style="color: #96a0aa;">clone to local
</span>git clone  https://github.com/ggerganov/llama.cpp

<span style="color: #c991e1;">cd</span> llama.cpp

<span style="color: #565575;"># </span><span style="color: #96a0aa;">vanilla compile
</span>make

<span style="color: #565575;"># </span><span style="color: #96a0aa;">other types of compile to supports GPU.
</span>
<span style="color: #565575;">## </span><span style="color: #96a0aa;">cuda
</span><span style="color: #565575;"># </span><span style="color: #96a0aa;">make LLAMA_CUBLAS=1
</span>
<span style="color: #565575;">## </span><span style="color: #96a0aa;">Apple device
</span><span style="color: #565575;"># </span><span style="color: #96a0aa;">LLAMA_METAL=1 make</span>
</pre>
</div>

<p>
If you execute this code on your server (e.g. in Ubuntu), you make see some errors due to the compiler, on which you should first update the <code>gcc</code> version by
</p>


<div class="org-src-container">
<pre class="src src-sh">sudo apt install gcc-Version gcc-Version-multilib g++-Version g++-Version-multilib
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-Version 50
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-Version 50
</pre>
</div>

<p>
you can replace "Version" in above commands into the gcc version like 9, and use the new compiler.
</p>

<p>
Before you make, you may need to execute <code>make clear</code> to clear the compile cache.
</p>


<p>
While the make procedure is DONE, then you will find a execute file named <code>main</code> in your directory, and you can check it with
</p>


<div class="org-src-container">
<pre class="src src-sh">
<span style="color: #c991e1;">export</span> <span style="color: #ffe9aa;">model_path</span>=<span style="color: #ffe9aa; font-style: italic;">"your llama model path"</span>
<span style="color: #c991e1;">export</span> <span style="color: #ffe9aa;">llama_path</span>=<span style="color: #ffe9aa; font-style: italic;">"your main file path"</span>

<span style="color: #c991e1;">export</span> <span style="color: #ffe9aa;">query</span>=<span style="color: #ffe9aa; font-style: italic;">"hello, whats your name?"</span>

$<span style="color: #ffe9aa;">llama_path</span> -m $<span style="color: #ffe9aa;">model_path</span> -p $<span style="color: #ffe9aa;">query</span>
</pre>
</div>

<p>
using <code>${llama_path} --help</code> to lookup all options:
</p>


<div class="org-src-container">
<pre class="src src-sh">usage: ./main <span style="color: #c66; font-weight: bold;">[</span>options<span style="color: #c66; font-weight: bold;">]</span>

options:
  -h, --help            show this help message and exit
  -i, --interactive     run<span style="color: #ff8080; font-weight: bold;"> in</span> interactive mode
  --interactive-first   run<span style="color: #ff8080; font-weight: bold;"> in</span> interactive mode and wait for input right away
  -ins, --instruct      run<span style="color: #ff8080; font-weight: bold;"> in</span> instruction mode <span style="color: #c66; font-weight: bold;">(</span>use with Alpaca models<span style="color: #c66; font-weight: bold;">)</span>
  --multiline-input     allows you to write or paste multiple lines without ending each<span style="color: #ff8080; font-weight: bold;"> in</span> <span style="color: #ffe9aa; font-style: italic;">'\'</span>
  -r PROMPT, --reverse-prompt PROMPT
                        halt generation at PROMPT, return control<span style="color: #ff8080; font-weight: bold;"> in</span> interactive mode
                        <span style="color: #c66; font-weight: bold;">(</span>can be specified more than once for multiple prompts<span style="color: #c66; font-weight: bold;">)</span><span style="color: #c991e1;">.</span>
  --color               colorise output to distinguish prompt and user input from generations
  -s SEED, --seed SEED  RNG seed <span style="color: #c66; font-weight: bold;">(</span>default: -1, use random seed for &lt; 0<span style="color: #c66; font-weight: bold;">)</span>
  -t N, --threads N     number of threads to use during computation <span style="color: #c66; font-weight: bold;">(</span>default: 40<span style="color: #c66; font-weight: bold;">)</span>
  -p PROMPT, --prompt PROMPT
                        prompt to start generation with <span style="color: #c66; font-weight: bold;">(</span>default: empty<span style="color: #c66; font-weight: bold;">)</span>
  -e                    process prompt escapes sequences <span style="color: #c66; font-weight: bold;">(</span>\n, \r, \t, <span style="color: #ffe9aa; font-style: italic;">\'</span>, <span style="color: #ffe9aa; font-style: italic;">\"</span>, <span style="color: #ffe9aa; font-style: italic;">\\</span><span style="color: #c66; font-weight: bold;">)</span>
  --prompt-cache FNAME  file to cache prompt state for faster startup <span style="color: #c66; font-weight: bold;">(</span>default: none<span style="color: #c66; font-weight: bold;">)</span>
  --prompt-cache-all    if specified, saves user input and generations to cache as well.
                        not supported with --interactive or other interactive options
  --prompt-cache-ro     if specified, uses the prompt cache but does not update it.
  --random-prompt       start with a randomized prompt.
  --in-prefix-bos       prefix BOS to user inputs, preceding the <span style="color: #fa8072;">`--in-prefix`</span> string
  --in-prefix STRING    string to prefix user inputs with <span style="color: #c66; font-weight: bold;">(</span>default: empty<span style="color: #c66; font-weight: bold;">)</span>
  --in-suffix STRING    string to suffix after user inputs with <span style="color: #c66; font-weight: bold;">(</span>default: empty<span style="color: #c66; font-weight: bold;">)</span>
  -f FNAME, --file FNAME
                        prompt file to start generation.
  -n N, --n-predict N   number of tokens to predict <span style="color: #c66; font-weight: bold;">(</span>default: -1, -1 = infinity, -2 = until context filled<span style="color: #c66; font-weight: bold;">)</span>
  -c N, --ctx-size N    size of the prompt context <span style="color: #c66; font-weight: bold;">(</span>default: 512<span style="color: #c66; font-weight: bold;">)</span>
  -b N, --batch-size N  batch size for prompt processing <span style="color: #c66; font-weight: bold;">(</span>default: 512<span style="color: #c66; font-weight: bold;">)</span>
  -gqa N, --gqa N       grouped-query attention factor <span style="color: #c66; font-weight: bold;">(</span>TEMP!!! use 8 for LLaMAv2 70B<span style="color: #c66; font-weight: bold;">)</span> <span style="color: #c66; font-weight: bold;">(</span>default: 1<span style="color: #c66; font-weight: bold;">)</span>
  -eps N, --rms-norm-eps N rms norm eps <span style="color: #c66; font-weight: bold;">(</span>TEMP!!! use 1e-5 for LLaMAv2<span style="color: #c66; font-weight: bold;">)</span> <span style="color: #c66; font-weight: bold;">(</span>default: 5.0e-06<span style="color: #c66; font-weight: bold;">)</span>
  --top-k N             top-k sampling <span style="color: #c66; font-weight: bold;">(</span>default: 40, 0 = disabled<span style="color: #c66; font-weight: bold;">)</span>
  --top-p N             top-p sampling <span style="color: #c66; font-weight: bold;">(</span>default: 0.9, 1.0 = disabled<span style="color: #c66; font-weight: bold;">)</span>
  --tfs N               tail free sampling, parameter z <span style="color: #c66; font-weight: bold;">(</span>default: 1.0, 1.0 = disabled<span style="color: #c66; font-weight: bold;">)</span>
  --typical N           locally typical sampling, parameter p <span style="color: #c66; font-weight: bold;">(</span>default: 1.0, 1.0 = disabled<span style="color: #c66; font-weight: bold;">)</span>
  --repeat-last-n N     last n tokens to consider for penalize <span style="color: #c66; font-weight: bold;">(</span>default: 64, 0 = disabled, -1 = ctx_size<span style="color: #c66; font-weight: bold;">)</span>
  --repeat-penalty N    penalize repeat sequence of tokens <span style="color: #c66; font-weight: bold;">(</span>default: 1.1, 1.0 = disabled<span style="color: #c66; font-weight: bold;">)</span>
  --presence-penalty N  repeat alpha presence penalty <span style="color: #c66; font-weight: bold;">(</span>default: 0.0, 0.0 = disabled<span style="color: #c66; font-weight: bold;">)</span>
  --frequency-penalty N repeat alpha frequency penalty <span style="color: #c66; font-weight: bold;">(</span>default: 0.0, 0.0 = disabled<span style="color: #c66; font-weight: bold;">)</span>
  --mirostat N          use Mirostat sampling.
                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.
                        <span style="color: #c66; font-weight: bold;">(</span>default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0<span style="color: #c66; font-weight: bold;">)</span>
  --mirostat-lr N       Mirostat learning rate, parameter eta <span style="color: #c66; font-weight: bold;">(</span>default: 0.1<span style="color: #c66; font-weight: bold;">)</span>
  --mirostat-ent N      Mirostat target entropy, parameter tau <span style="color: #c66; font-weight: bold;">(</span>default: 5.0<span style="color: #c66; font-weight: bold;">)</span>
  -l TOKEN_ID<span style="color: #c66; font-weight: bold;">(</span>+/-<span style="color: #c66; font-weight: bold;">)</span>BIAS, --logit-bias TOKEN_ID<span style="color: #c66; font-weight: bold;">(</span>+/-<span style="color: #c66; font-weight: bold;">)</span>BIAS
                        modifies the likelihood of token appearing<span style="color: #ff8080; font-weight: bold;"> in</span> the completion,
                        i.e. <span style="color: #fa8072;">`--logit-bias 15043+1`</span> to increase likelihood of token <span style="color: #ffe9aa; font-style: italic;">' Hello'</span>,
                        or <span style="color: #fa8072;">`--logit-bias 15043-1`</span> to decrease likelihood of token <span style="color: #ffe9aa; font-style: italic;">' Hello'</span>
  --grammar GRAMMAR     BNF-like grammar to constrain generations <span style="color: #c66; font-weight: bold;">(</span>see samples<span style="color: #ff8080; font-weight: bold;"> in</span> grammars/ dir<span style="color: #c66; font-weight: bold;">)</span>
  --grammar-file FNAME  file to read grammar from
  --cfg-negative-prompt PROMPT
                        negative prompt to use for guidance. <span style="color: #c66; font-weight: bold;">(</span>default: empty<span style="color: #c66; font-weight: bold;">)</span>
  --cfg-scale N         strength of guidance <span style="color: #c66; font-weight: bold;">(</span>default: 1.000000, 1.0 = disable<span style="color: #c66; font-weight: bold;">)</span>
  --rope-scale N        RoPE context linear scaling factor, inverse of --rope-freq-scale <span style="color: #c66; font-weight: bold;">(</span>default: 1<span style="color: #c66; font-weight: bold;">)</span>
  --rope-freq-base N    RoPE base frequency, used by NTK-aware scaling <span style="color: #c66; font-weight: bold;">(</span>default: 10000.0<span style="color: #c66; font-weight: bold;">)</span>
  --rope-freq-scale N   RoPE frequency linear scaling factor, inverse of --rope-scale <span style="color: #c66; font-weight: bold;">(</span>default: 1<span style="color: #c66; font-weight: bold;">)</span>
  --ignore-eos          ignore end of stream token and continue generating <span style="color: #c66; font-weight: bold;">(</span>implies --logit-bias 2-inf<span style="color: #c66; font-weight: bold;">)</span>
  --no-penalize-nl     <span style="color: #ff8080; font-weight: bold;"> do</span> not penalize newline token
  --memory-f32          use f32 instead of f16 for memory key+value <span style="color: #c66; font-weight: bold;">(</span>default: disabled<span style="color: #c66; font-weight: bold;">)</span>
                        not recommended: doubles context memory required and no measurable increase<span style="color: #ff8080; font-weight: bold;"> in</span> quality
  --temp N              temperature <span style="color: #c66; font-weight: bold;">(</span>default: 0.8<span style="color: #c66; font-weight: bold;">)</span>
  --perplexity          compute perplexity over each ctx window of the prompt
  --hellaswag           compute HellaSwag score over random tasks from datafile supplied with -f
  --hellaswag-tasks N   number of tasks to use when computing the HellaSwag score <span style="color: #c66; font-weight: bold;">(</span>default: 400<span style="color: #c66; font-weight: bold;">)</span>
  --keep N              number of tokens to keep from the initial prompt <span style="color: #c66; font-weight: bold;">(</span>default: 0, -1 = all<span style="color: #c66; font-weight: bold;">)</span>
  --chunks N            max number of chunks to process <span style="color: #c66; font-weight: bold;">(</span>default: -1, -1 = all<span style="color: #c66; font-weight: bold;">)</span>
  --mlock               force system to keep model<span style="color: #ff8080; font-weight: bold;"> in</span> RAM rather than swapping or compressing
  --no-mmap            <span style="color: #ff8080; font-weight: bold;"> do</span> not memory-map model <span style="color: #c66; font-weight: bold;">(</span>slower load but may reduce pageouts if not using mlock<span style="color: #c66; font-weight: bold;">)</span>
  --numa                attempt optimizations that help on some NUMA systems
                        <span style="color: #ff8080; font-weight: bold;">if</span> run without this previously, it is recommended to drop the system page cache before using this
                        see https://github.com/ggerganov/llama.cpp/issues/1437
  --mtest               compute maximum memory usage
  --export              export the computation graph to <span style="color: #ffe9aa; font-style: italic;">'llama.ggml'</span>
  --verbose-prompt      print prompt before generation
  --simple-io           use basic IO for better compatibility<span style="color: #ff8080; font-weight: bold;"> in</span> subprocesses and limited consoles
  --lora FNAME          apply LoRA adapter <span style="color: #c66; font-weight: bold;">(</span>implies --no-mmap<span style="color: #c66; font-weight: bold;">)</span>
  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter
  -m FNAME, --model FNAME
                        model path <span style="color: #c66; font-weight: bold;">(</span>default: models/7B/ggml-model.bin<span style="color: #c66; font-weight: bold;">)</span>
</pre>
</div>

<p>
If you have no LLAMA checkpoints, you can access from the LLAMA page, or download the quantization version from <a href="https://huggingface.co/TheBloke">this man</a>, what I have used is the llama2-7B with int8 <a href="https://huggingface.co/TheBloke/Llama-2-7B-GGML/tree/main">here</a> such as
</p>


<div id="orgd4687a3" class="figure">
<p><img src="./images/screenshot_20230816_180642.png" alt="screenshot_20230816_180642.png">
</p>
</div>


<p>
Beyond the interactive mode of llama.cpp, you might hope to use it as a API, it is not supports until now, but I simply write a synchronous API for my research and experiements. where the python code can be shown as:
</p>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #ff8080; font-weight: bold;">import</span> subprocess

<span style="color: #ff8080; font-weight: bold;">def</span> <span style="color: #c991e1;">inference_llama2</span><span style="color: #c66; font-weight: bold;">(</span>sent<span style="color: #c66; font-weight: bold;">)</span>:
    <span style="color: #ffe9aa;">llama</span>=<span style="color: #ffe9aa; font-style: italic;">"/home/liangzi/llama.cpp/main"</span>
    <span style="color: #ffe9aa;">model</span>=<span style="color: #ffe9aa; font-style: italic;">"/home/liangzi/models/llama2/llama-2-7b-chat.ggmlv3.q8_0.bin"</span>
    <span style="color: #ffe9aa;">scripts</span>=f<span style="color: #ffe9aa; font-style: italic;">"""
    </span>{llama}<span style="color: #ffe9aa; font-style: italic;"> -m </span>{model}<span style="color: #ffe9aa; font-style: italic;"> -p "Please response to the question of user. User: </span>{sent}<span style="color: #ffe9aa; font-style: italic;"> Answer: "
    """</span>
    <span style="color: #ffe9aa;">result</span> = subprocess.run<span style="color: #c66; font-weight: bold;">(</span>scripts, shell=<span style="color: #aaffe4;">True</span>, capture_output=<span style="color: #aaffe4;">True</span><span style="color: #c66; font-weight: bold;">)</span>.stdout.decode<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"utf8"</span><span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #ff8080; font-weight: bold;">if</span> <span style="color: #ffe9aa; font-style: italic;">"Answer: "</span> <span style="color: #ff8080; font-weight: bold;">in</span> result:
        <span style="color: #ffe9aa;">result</span>=result.split<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"Answer: "</span><span style="color: #c66; font-weight: bold;">)[</span>1<span style="color: #c66; font-weight: bold;">]</span>
    <span style="color: #ff8080; font-weight: bold;">if</span> <span style="color: #ffe9aa; font-style: italic;">"User"</span> <span style="color: #ff8080; font-weight: bold;">in</span> result:
        <span style="color: #ffe9aa;">result</span>=result.split<span style="color: #c66; font-weight: bold;">(</span><span style="color: #ffe9aa; font-style: italic;">"User"</span><span style="color: #c66; font-weight: bold;">)[</span>0<span style="color: #c66; font-weight: bold;">]</span>
    <span style="color: #c991e1;">print</span><span style="color: #c66; font-weight: bold;">(</span>result<span style="color: #c66; font-weight: bold;">)</span>
    <span style="color: #ff8080; font-weight: bold;">return</span> result
</pre>
</div>

<p>
This is very simple, you can write your own version to support more complicated functions.
</p>
</div>
</div>
<div id="outline-container-org54e2faf" class="outline-3">
<h3 id="org54e2faf"><span class="section-number-3">3.2.</span> By candle, to obtain llama &amp; llama2 &amp; falcon</h3>
<div class="outline-text-3" id="text-3-2">
<p>
candle is a ML framework in Rust, by huggingface.
</p>

<ul class="org-ul">
<li>OS: Linux recommanded</li>
<li>Device:  GPU recommanded</li>
<li>link: <a href="https://github.com/huggingface/candle">https://github.com/huggingface/candle</a></li>
<li>proxy: not required</li>
<li>is official: NA</li>
</ul>


<p>
Compared to llama.cpp, using candle is easier.
</p>

<ol class="org-ol">
<li>you need have a recent stable <code>rust</code> environment in your system to compile candle.</li>
<li>clone the repository with: git clone <a href="https://github.com/huggingface/candle">https://github.com/huggingface/candle</a></li>
<li>running one of the example commands:</li>
</ol>

<div class="org-src-container">
<pre class="src src-sh">cargo run --example whisper --release
cargo run --example llama --release
cargo run --example falcon --release
cargo run --example bert --release
cargo run --example bigcode --release
cargo run --example stable-diffusion --release --features image -- --prompt <span style="color: #ffe9aa; font-style: italic;">"a rusty robot holding a fire torch"</span>
</pre>
</div>

<p>
When I use candle, I face the problem:
</p>


<div class="org-src-container">
<pre class="src src-sh">Error: request error: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer.json: status code 401

Caused by:
    https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer.json: status code 401
</pre>
</div>


<p>
which is caused by the access permission of pre-trained models.
</p>

<p>
Take llama2 as an example, you may should first acess this model in <a href="https://huggingface.co/meta-llama/Llama-2-7b-hf">huggingface's website</a> :
</p>


<div id="orgb384afe" class="figure">
<p><img src="./images/screenshot_20230816_181741.png" alt="screenshot_20230816_181741.png">
</p>
</div>

<p>
you cannot use this model until you got a similar granted texts as mine.
</p>



<p>
Then, install <code>huggingface_hub</code> , where you might install it already if your python environment has transformers or other huggingface's packages.
</p>

<p>
Use  <code>huggingface-cli login</code> and insert your access token. If you have no token just lookup <a href="https://huggingface.co/settings/tokens">here</a>.
</p>

<p>
Above solution comes from <a href="https://github.com/huggingface/candle/issues/350">this issue</a>.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr class="Solid"> <div class="info"> <span class="author">Author: Zi Liang (<a href="mailto:liangzid@stu.xjtu.edu.cn">liangzid@stu.xjtu.edu.cn</a>)</span> <span class="date">Create Date: Sat Aug 12 18:04:48 2023</span> <span class="date">Last modified: 2025-09-27 Sat 19:50</span> <span>Creator: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 30.2 (<a href="https://orgmode.org">Org</a> mode 9.7.11)</span> </div>
</div>
</body>
</html>
