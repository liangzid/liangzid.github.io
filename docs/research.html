<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2025-12-04 四 11:15 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Zi Liang (Research Page)</title>
<meta name="author" content="liangzid" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/css/styles.css" /> <link rel="stylesheet" type="text/css" href="/css/htmlize.css" /> <script src="/scripts/script.js"></script> <script src="/scripts/toc.js"></script>
</head>
<body>
<div id="preamble" class="status">
<nav class="nav"> <a href="/index.html" class="button">Home</a> <a href="/about.html" class="button">About</a> <a href="/sitemap.html" class="button">Sitemaps</a> </nav> <hr>
</div>
<div id="content" class="content">
<h1 class="title">Zi Liang (Research Page)</h1>

<div id="org552bffc" class="figure">
<p><img src="images/danjin.jpg" alt="danjin.jpg">
</p>
</div>
<div id="outline-container-org75c3db7" class="outline-2">
<h2 id="org75c3db7"><span class="section-number-2">1.</span> Introducing Myself</h2>
<div class="outline-text-2" id="text-1">
<p>
My name is Zi Liang, now a PhD student in the <a href="https://www.astaple.com/">Astaple Group</a> of Hong Kong Polytechnic University (PolyU). My supervisor is Prof. <a href="https://haibohu.org/">Haibo Hu</a>. I begin my research in Xi'an Jiaotong University, under the supervision of Prof. <a href="https://gr.xjtu.edu.cn/web/phwang">Pinghui Wang</a> and <a href="https://www.linkedin.com/in/ruofei">Ruofei (Bruce) Zhang</a>.
I also work closely with <a href="https://yywang.netlify.app/">Yanyun Wang</a> (HKUST-Guangzhou), <a href="https://hongzongli-cs.github.io/">Hongzong Li</a> (HK CityU),  <a href="https://xuanl17.github.io/">Xuan Liu </a>(UCSD), <a href="https://scholar.google.com.hk/citations?user=XzO2dV0AAAAJ&amp;hl=zh-CN">Nuo Xu</a> (Limix AI), <a href="https://scholar.google.com.hk/citations?user=Wd5IdkMAAAAJ&amp;hl=zh-TW">Shuo Zhang</a> (Huawei), <a href="https://scholar.google.com/citations?user=spRkQ2oAAAAJ&amp;hl=en">Yaxin Xiao</a> (HK PolyU), and <a href="https://xinweizhang1998.github.io/">Xinwei Zhang</a> (HK PolyU).
</p>

<p>
I specialize in analyzing the potential risks inherent in language models, with a focus on <b>understanding why and how neural networks function and identifying vulnerabilities within them</b>. My research is driven by a deep curiosity to <i>uncover the mechanisms behind these models and to address the security challenges they present</i>.
</p>

<p>
My work can be categorized into two main areas:
</p>

<ul class="org-ul">
<li><b>Enhancing Understanding of Models and Learning Processes</b>: I aim to explain the root causes of issues in AI systems, examining how problems arise during model training and inference, and what they imply for the broader field of machine learning.</li>
<li><b>Uncovering New Threats and Developing Defenses</b>: I conduct comprehensive evaluations of popular AI services and techniques, combining in-depth theoretical analysis with practical experimentation;</li>
</ul>
</div>
<div id="outline-container-orgd6378d3" class="outline-3">
<h3 id="orgd6378d3"><span class="section-number-3">1.1.</span> Contact Me</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>GitHub: <a href="https://github.com/liangzid">https://github.com/liangzid</a></li>
<li>Mail: zi1415926.liang@connect.polyu.hk</li>
<li><a href="https://scholar.google.com/citations?user=pzrGwvMAAAAJ&amp;hl=zh-CN">Google Scholar</a></li>
<li>Wechat: paperacceptplease</li>
<li><p>
Face-to-Face: Free feel to tell with me when you meet me.
</p>

<p>
I look like this:
</p>


<div id="orgb9bef83" class="figure">
<p><img src="./images/screenshot_20251204_111507.png" alt="screenshot_20251204_111507.png"> 
</p>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org191e16a" class="outline-2">
<h2 id="org191e16a"><span class="section-number-2">2.</span> Publications</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org0e8b7b2" class="outline-3">
<h3 id="org0e8b7b2"><span class="section-number-3">2.1.</span> As the First Author</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li><b><i>Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary.</i></b> <b><code>Zi Liang</code></b>, Zhiyao Wu, Haoyang Shang, Yulin Jin, Qingqing Ye, Huadi Zheng, Peizhao Hu, and Haibo Hu. -&lt;Arxiv Preprint'25&gt; [<a href="https://www.arxiv.org/abs/2510.03271">Paper</a>] [<a href="https://github.com/liangzid/DPS">Code</a>]</li>
<li><i><b>The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution</b></i> Fei Gu, <b><code>Zi Liang</code></b>, Hongzong Li, and Jiahao Ma. -&lt;Arxiv Preprint'25&gt; [<a href="https://arxiv.org/abs/2509.23261">Paper</a>] [Code]</li>
<li><i><b>Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</b></i> <b><code>Zi Liang</code></b>, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, and Haibo Hu -&lt;NeurIPS'25 Spotlight&gt; [<a href="https://arxiv.org/abs/2509.23041">Paper</a>] [<a href="https://github.com/liangzid/VirusInfectionAttack">Code</a>]</li>
<li><i><b>How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework</b></i>. <b><code>Zi Liang</code></b>, Liantong Yu, Shiyu Zhang, Qingqing Ye, and Haibo Hu -&lt;AAAI'26&gt; [<a href="https://arxiv.org/abs/2507.19219">Paper</a>] [<a href="https://github.com/liangzid/ArxivRoll/">Code</a>] [<a href="https://arxivroll.moreoverai.com/">Website</a>]</li>
<li><i><b>"Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation.</b></i> <b><code>Zi Liang</code></b>, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, and Haibo Hu - &lt;ACL'25 main&gt; [<a href="https://arxiv.org/abs/2409.02718">Paper</a>] [<a href="https://github.com/liangzid/LoRD-MEA">Code</a>]</li>
<li><i><b>Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?</b></i> <b><code>Zi Liang</code></b>, Haibo Hu, Qingqing Ye, Yaxin Xiao, and Ronghua Li. -&lt;ICML'25&gt; [<a href="https://arxiv.org/abs/2505.12871">Paper</a>] [<a href="https://github.com/liangzid/LoRA-sSecurity">Code</a>]</li>
<li><i><b>Exploring Intrinsic Alignments within Text Corpus.</b></i> <b><code>Zi Liang</code></b>, Pinghui Wang, Ruofei Zhang, Haibo Hu, &#x2026; - &lt;AAAI'25, Oral&gt; [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/34957">Paper</a>] [<a href="https://github.com/liangzid/TEMP">Code</a>]</li>
<li><i><b>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models.</b></i> <b><code>Zi Liang</code></b>, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li - &lt;Preprint&gt; [<a href="https://arxiv.org/abs/2408.02416">Paper</a>][<a href="https://github.com/liangzid/PromptExtractionEval">Code</a>]</li>
<li><i><b>MERGE: Fast Private Text Generation.</b></i>  <b><code>Zi Liang</code></b>, P Wang, R Zhang, Nuo Xu, Shuo Zhang, Lifeng Xing… - &lt;AAAI'24&gt; [<a href="https://arxiv.org/abs/2305.15769">Paper</a>] [<a href="https://github.com/liangzid/MERGE">Code</a>]</li>
</ul>
</div>
</div>
<div id="outline-container-orgf3c5cdd" class="outline-3">
<h3 id="orgf3c5cdd"><span class="section-number-3">2.2.</span> As a Coauthor</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li><i>Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks</i>. Yaxin Xiao, Qingqing Ye, <b><code>Zi Liang</code></b>, Haoyang Li, Ronghua Li, Huadi Zheng, and Haibo Hu - &lt;AAAI'26&gt;</li>
<li><i><b>Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy</b></i>. Yaxin Xiao, Qingqing Ye, Li Hu, Huadi Zheng, Haibo Hu, <b><code>Zi Liang</code></b>, Haoyang Li, Yijie Jiao. - &lt;ICCV'25&gt;</li>
<li><i><b>Unlocking High-Fidelity Learning: Towards Neuron-Grained Model Extraction</b></i>. Yaxin Xiao, Haibo Hu, Qingqing Ye, Li Tang, <b><code>Zi Liang</code></b>, Huadi Zheng - &lt;IEEE TDSC'25&gt;</li>
<li><i><b>New Paradigm of Adversarial Training: Breaking Inherent Trade-Off between Accuracy and Robustness via Dummy Classes.</b></i> Yanyun Wang, Li Liu, <b><code>Zi Liang</code></b>, Qingqing Ye, Haibo Hu. - &lt;Preprint'25&gt;</li>
<li><i><b>Cross-Modal 3D Representation with Multi-View Images and Point Clouds.</b></i> Ziyang Zhou, Pinghui Wang,*~Zi Liang~*, Haitao Bai, Ruofei Zhang. - &lt;CVPR'25&gt;</li>
<li><i><b>How Vital is the Jurisprudential Relevance: Law Article Intervened Legal Case Retrieval and Matching.</b></i> Nuo Xu, Pinghui Wang, <b><code>Zi Liang</code></b>, Junzhou Zhao, Xiaohong Guan &lt;Preprint'25&gt;</li>
<li><i><b>PAIR: Pre-denosing Augmented Image Retrieval Model for Defending Adversarial Patches.</b></i> Ziyang Zhou, Pinghui Wang, <b><code>Zi Liang</code></b>, Rruofei Zhang, Haitao Bai - &lt;MM'24&gt;</li>
<li><i><b>TSFool: Crafting Highly-Imperceptible Adversarial Time Series through Multi-Objective Attack.</b></i> Yanyun Wang, Dehui Du, Haibo Hu,  <b><code>Zi Liang</code></b>, Yuanhao Liu - &lt;ECAI'24&gt;</li>
<li><i><b>Multi-action dialog policy learning from logged user feedback.</b></i> Shuo Zhang, Junzhou Zhao, Pinghui Wang, T Wang,  <b><code>Zi Liang</code></b>, Jing Tao… - &lt;AAAI'23&gt;</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org28cb368" class="outline-2">
<h2 id="org28cb368"><span class="section-number-2">3.</span> Introducing My Recent Research</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org4c1b0bc" class="outline-3">
<h3 id="org4c1b0bc"><span class="section-number-3">3.1.</span> [Preprint'25] Constructing LLM Decision Boundary!</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li><p>
Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary [Preprint'25]
</p>


<div id="orgabeb520" class="figure">
<p><img src="./images/screenshot_20250927_204135.png" alt="screenshot_20250927_204135.png"> 
</p>
</div>

<p>
This paper focues on a mission nearly impossible: to construct the decision boundary for modern LLMs! We:
</p>
<ul class="org-ul">
<li>formalize the decision boudnary of LLMs as a combined multi-class classification</li>
<li>propose decision potential surface (DPS) which is defined with the decision potential function, and prove that the 0-height isohypse (contour lines) in it is equvilent to decision boundary of LLMs</li>
<li>propose K-DPS, an approximation of DPS which only reuqires K time of sampling for each input point, and investigate the error bounds theoretically and empirically.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgda6d056" class="outline-3">
<h3 id="orgda6d056"><span class="section-number-3">3.2.</span> [Preprint'25] Influences of AI-Coding Tools on Software Engineering</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li><p>
The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution [Preprint'25]
</p>

<p>
This paper investigate <i>whether current popular AI coding tools (e.g., Cursor, Copilot) will influence the <b>evolution</b> of software ecosystem</i>.
We investigate the correlation between coding performance and popularity on vital software factors under the generation of LLMs, revealing a Matthew effect, i.e., AI coding tools might cause those not popular factors less popular. We investigate two factors in software ecosystem, the language and the programming framework.
</p></li>
</ul>


<div id="orgd7f8bbb" class="figure">
<p><img src="./images/screenshot_20250927_203308.png" alt="screenshot_20250927_203308.png">
</p>
</div>

<p>
PS: While focusing on LLM safety in my PhD procedure, I spent a lot of time on software engineering during my master stage, which really makes me think the influence of AI coding tools on software engineering.
</p>
</div>
</div>
<div id="outline-container-org7586528" class="outline-3">
<h3 id="org7586528"><span class="section-number-3">3.3.</span> [AAAI'26] Benchmark LLM when Considering Cheating</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework. [Preprint'25]</li>
</ul>


<div id="orgabb844c" class="figure">
<p><img src="./images/screenshot_20250927_202632.png" alt="screenshot_20250927_202632.png">  
</p>
</div>

<p>
Inspired by the OTP in cryptography, we propose a new benchmark (which is also a dynamic benchmark), named ArxivRollBench, which can automatically generate test cases from new articles on ArXiv with a high quality.
Also, we propose a new evaluation framework to quantify the propertion of cheating.
We use this benchmark to systemtically investigate the practical performance of current LLMs.
</p>

<p>
Leaderboard Path: <a href="https://arxivroll.moreoverai.com">https://arxivroll.moreoverai.com</a>
</p>

<p>
PS: I will maintain and update the leaderboard every six month. 
</p>
</div>
</div>
<div id="outline-container-org26247f4" class="outline-3">
<h3 id="org26247f4"><span class="section-number-3">3.4.</span> [NeurIPS'25 Spotlight] Security Analysis on Synthetic Data Based Training</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li><p>
Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data [NeurIPS'25 Spotlight]
</p>


<div id="org2127a1d" class="figure">
<p><img src="./images/screenshot_20250927_201328.png" alt="screenshot_20250927_201328.png"> 
</p>
</div></li>
</ul>

<p>
Synthetic data denotes <i>artificial samples</i> generated by models, which is important for modern LLM training and distillation.
In this paper, we reveal that current training paradigm exhibits strong resistance to mainstream attacks thanks to the different distribution patterns.
Moreover, we propose a new attack (Virus Infection Attack, VIA) to enable the propagation of current poisoning under synthetic-data-based training and distillation.
This is the first study to investigate synthetic data's security, and also, the first attack which enables the infection ability of poisoning.
</p>
</div>
</div>
<div id="outline-container-orgfe7878e" class="outline-3">
<h3 id="orgfe7878e"><span class="section-number-3">3.5.</span> [ICML'25] LoRA Robustness Analysis</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li><p>
Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks? [ICML'25]
</p>

<p>
This paper investigates the robustness of LoRA compared with the full fine-tuning.
We use NTK (neural tangent kernel) to model the kernel function and the difference between LoRA and full fine-tuning. We find that:
</p>
<ol class="org-ol">
<li>LoRA is more sensitive than full fine-tuning against untargeted poisoning attack;</li>
<li>LoRA is more robust than full fine-tuning against backdoor poisoning attack.</li>
</ol></li>
</ul>


<p>
We also reveal that the <b>rank</b> and the initialization variance in LoRA will influence LoRA's robustness.
</p>
</div>
</div>
<div id="outline-container-orgd8ebcee" class="outline-3">
<h3 id="orgd8ebcee"><span class="section-number-3">3.6.</span> [ACL'25] LLM's Model Extraction (Stealing) Attacks</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>"Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation [ACL'25 Main Paper]</li>
</ul>


<div id="org81550b1" class="figure">
<p><img src="./images/screenshot_20250309_221216.png" alt="screenshot_20250309_221216.png">
</p>
</div>

<p>
This paper investigates an interesting question: <b>whether MLE (i.e., the cross-entropy loss) is compatible with stealing an LLM learned via RL-based methods?</b> In other words, it aims to explore <b>how to <i>effectively</i> and <i>efficiently</i> steal LLMs.</b>
</p>

<p>
We demonstrate that: <i>i)</i> MLE can truly be used to steal LLMs, but <i>ii)</i> it suffers from a high complexity of query times.
</p>

<p>
We propose a new RL-based method for this task and show its effectiveness and intrinsic watermark resistance.
</p>
</div>
</div>
<div id="outline-container-orgeb779e7" class="outline-3">
<h3 id="orgeb779e7"><span class="section-number-3">3.7.</span> [Preprint'25] Prompt Leakage in LLMs</h3>
<div class="outline-text-3" id="text-3-7">
<ul class="org-ul">
<li>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models [Preprint]</li>
</ul>


<div id="orgd02c4db" class="figure">
<p><img src="./images/screenshot_20250309_221310.png" alt="screenshot_20250309_221310.png">
</p>
</div>


<div id="orgd86cf70" class="figure">
<p><img src="./images/screenshot_20250309_221323.png" alt="screenshot_20250309_221323.png">
</p>
</div>

<p>
This paper uncovers the threat of <b>prompt leakage</b> on customized prompt-based services, such as OpenAI's GPTs. It aims to answer three questions:
</p>
<ol class="org-ol">
<li>Can LLM's alignments defend against prompt extraction attacks?</li>
<li>How do LLMs leak their prompts?</li>
<li>Which factors of prompts and LLMs lead to such leakage?</li>
</ol>


<p>
We provide a comprehensive and systemic evaluation to answer question 1 and 3, and propose two hypotheses with experimental validation for question 2. We also propose several easy-to-adopt defending strategies based on our discovery.
</p>

<p>
Click <a href="https://arxiv.org/abs/2408.02416">here</a> if you are also interested in this research.
</p>
</div>
</div>
<div id="outline-container-org348cd9c" class="outline-3">
<h3 id="org348cd9c"><span class="section-number-3">3.8.</span> [AAAI'24] Private Inference in LLMs</h3>
<div class="outline-text-3" id="text-3-8">
<ul class="org-ul">
<li>MERGE: Fast Private Text Generation [AAAI'24]</li>
</ul>


<div id="orgfe20357" class="figure">
<p><img src="./images/screenshot_20250309_221412.png" alt="screenshot_20250309_221412.png">
</p>
</div>

<p>
This paper proposes a new privacy-preserving inference framework for current transformer-based generative language models based on Secret Sharing and Multi-party Security Computation (MPC). It is also the <b>first</b> private inference framework specifically designed for NLG models. 10x of speedup is provided via our propose method.
</p>

<p>
If you are curious about how cryptography protects the privacy of user contents and models and how we optimize the inference procedure, click <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29964">here</a> for more details.
</p>
</div>
</div>
<div id="outline-container-orgfef2fe2" class="outline-3">
<h3 id="orgfef2fe2"><span class="section-number-3">3.9.</span> [AAAI'25] Mining the Sources of AI Alignments</h3>
<div class="outline-text-3" id="text-3-9">
<ul class="org-ul">
<li><p>
Exploring Intrinsic Alignments within Text Corpus. [AAAI'25]
</p>


<div id="org4b49ed4" class="figure">
<p><img src="./images/screenshot_20250309_222112.png" alt="screenshot_20250309_222112.png"> 
</p>
</div>

<p>
This paper explores the possibility of utilizing the intrinsic signal within raw dialogue texts as the feedback signal for current LLMs. Under a prior distribution of text corpus, we propose a method to sample potentially safer responses without human annotation information.
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgfd40690" class="outline-2">
<h2 id="orgfd40690"><span class="section-number-2">4.</span> Experiences</h2>
<div class="outline-text-2" id="text-4">
<ol class="org-ol">
<li>2016.09-2020.06: Bachelor Degree, in Northeastern University, on <i>cybernetics (i.e., automation, or the Control Theory)</i>;</li>
<li>2020.09-2023.06: Master Degree, in the iMiss Group of Xi'an Jiaotong University, on <i>software engineering</i> and research for <i>Conversational AI</i> and <i>NLP Security</i>;</li>
<li>2023.11-now: PhD Student, in the The Hong Kong Polytechnic University in Hong Kong. Research of interests: <i>AI safety, privacy and security</i> and <i>Natural Language Processing</i>.</li>
</ol>
</div>
</div>
<div id="outline-container-org7b4c6e9" class="outline-2">
<h2 id="org7b4c6e9"><span class="section-number-2">5.</span> Contact Me</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>GitHub: <a href="https://github.com/liangzid">https://github.com/liangzid</a></li>
<li>Mail: zi1415926.liang@connect.polyu.hk</li>
<li><a href="https://scholar.google.com/citations?user=pzrGwvMAAAAJ&amp;hl=zh-CN">Google Scholar</a></li>
<li>Wechat: paperacceptplease</li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr class="Solid"> <div class="info"> <span class="author">Author: liangzid (<a href="mailto:2273067585@qq.com">2273067585@qq.com</a>)</span> <span class="date">Create Date: Fri Sep 10 14:23:46 2021</span> <span class="date">Last modified: 2025-12-04 四 11:15</span> <span>Creator: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 30.2 (<a href="https://orgmode.org">Org</a> mode 9.7.11)</span> </div>
</div>
</body>
</html>
