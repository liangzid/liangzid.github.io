<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2025-03-16 Sun 20:23 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Zi Liang (Research Page)</title>
<meta name="author" content="liangzid" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/css/styles.css" /> <link rel="stylesheet" type="text/css" href="/css/htmlize.css" /> <script src="/scripts/script.js"></script> <script src="/scripts/toc.js"></script>
</head>
<body>
<div id="preamble" class="status">
<nav class="nav"> <a href="/index.html" class="button">Home</a> <a href="/sitemap.html" class="button">Sitemaps</a> </nav> <hr>
</div>
<div id="content" class="content">
<h1 class="title">Zi Liang (Research Page)</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org97412e6">1. Introduction</a>
<ul>
<li><a href="#org91311b7">1.1. Introducing Myself</a></li>
<li><a href="#orgab4114f">1.2. Contact Me</a></li>
<li><a href="#org115af1f">1.3. Introducing My Recent Research</a>
<ul>
<li><a href="#org58f2e20">1.3.1. LLM's Model Extraction (Stealing) Attacks</a></li>
<li><a href="#orgc21c6f6">1.3.2. Prompt Leakage in LLMs</a></li>
<li><a href="#org4528183">1.3.3. Private Inference in LLMs</a></li>
<li><a href="#orgde7c4e9">1.3.4. Mining the Sources of AI Alignments</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0d931cc">2. Experiences</a></li>
<li><a href="#orgd1cc1b6">3. Publications</a>
<ul>
<li><a href="#org6b9883b">3.1. As the First Author</a></li>
<li><a href="#orgcc1054c">3.2. As the main contributor</a></li>
<li><a href="#org0044d3f">3.3. As one of co-authors</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="org769e54b" class="figure">
<p><img src="images/danjin.jpg" alt="danjin.jpg">
</p>
</div>

<div id="outline-container-org97412e6" class="outline-2">
<h2 id="org97412e6"><span class="section-number-2">1.</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org91311b7" class="outline-3">
<h3 id="org91311b7"><span class="section-number-3">1.1.</span> Introducing Myself</h3>
<div class="outline-text-3" id="text-1-1">
<p>
My name is Zi Liang, now a PhD student in the <a href="https://www.astaple.com/">Astaple Group</a> of Hong Kong Polytechnic University (PolyU). My supervisor is Prof. <a href="https://haibohu.org/">Haibo Hu</a>. I begin my research in Xi'an Jiaotong University, under the supervision of Prof. <a href="https://gr.xjtu.edu.cn/web/phwang">Pinghui Wang</a> and <a href="https://www.linkedin.com/in/ruofei">Ruofei (Bruce) Zhang</a>.
I also work closely with <a href="https://yywang.netlify.app/">Yanyun Wang</a> (HKUST-Guangzhou), <a href="https://scholar.google.com/citations?user=spRkQ2oAAAAJ&amp;hl=en">Yaxin Xiao</a> (HK PolyU)  <a href="https://scholar.google.com.hk/citations?user=XzO2dV0AAAAJ&amp;hl=zh-CN">Nuo Xu</a> (Huawei), <a href="https://scholar.google.com.hk/citations?user=Wd5IdkMAAAAJ&amp;hl=zh-TW">Shuo Zhang</a> (XJTU), <a href="https://xuanl17.github.io/">Xuan Liu</a>, and <a href="https://xinweizhang1998.github.io/">Xinwei Zhang</a> (HK PolyU).
</p>

<p>
I specialize in analyzing the potential risks inherent in language models, with a focus on <b>understanding why and how neural networks function and identifying vulnerabilities within them</b>. My research is driven by a deep curiosity to <i>uncover the mechanisms behind these models and to address the security challenges they present</i>.
</p>

<p>
My work can be categorized into two main areas:
</p>

<ul class="org-ul">
<li><b>Uncovering New Threats and Developing Defenses</b>: I conduct comprehensive evaluations of popular AI services and techniques, combining in-depth theoretical analysis with practical experimentation]);</li>
<li><b>Enhancing Understanding of Models and Learning Processes</b>: I aim to explain the root causes of safety issues in AI systems, examining how these problems arise during model training and inference, and what they imply for the broader field of machine learning.</li>
</ul>

<p>
In addition to my research, I have extensive experience in <i>natural language processing (NLP)</i>, particularly in building conversational AI systems, which I have been actively involved in since 2019. More recently, starting in 2024, I have developed a strong interest in the future of AI, particularly in the application of reinforcement learning (RL) to advance the capabilities and safety of intelligent systems.
</p>
</div>
</div>

<div id="outline-container-orgab4114f" class="outline-3">
<h3 id="orgab4114f"><span class="section-number-3">1.2.</span> Contact Me</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>GitHub: <a href="https://github.com/liangzid">https://github.com/liangzid</a></li>
<li>MAIL: zi1415926.liang@connect.polyu.hk</li>
<li><a href="https://scholar.google.com/citations?user=pzrGwvMAAAAJ&amp;hl=zh-CN">Google Scholar</a></li>
</ul>
</div>
</div>

<div id="outline-container-org115af1f" class="outline-3">
<h3 id="org115af1f"><span class="section-number-3">1.3.</span> Introducing My Recent Research</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-org58f2e20" class="outline-4">
<h4 id="org58f2e20"><span class="section-number-4">1.3.1.</span> LLM's Model Extraction (Stealing) Attacks</h4>
<div class="outline-text-4" id="text-1-3-1">
<ul class="org-ul">
<li>"Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation [Preprint]</li>
</ul>


<div id="org58429ef" class="figure">
<p><img src="./images/screenshot_20250309_221216.png" alt="screenshot_20250309_221216.png">
</p>
</div>

<p>
This paper investigates an interesting question: <b>whether MLE (i.e., the cross-entropy loss) is compatible with stealing an LLM learned via RL-based methods?</b> In other words, it aims to explore <b>how to <i>effectively</i> and <i>efficiently</i> steal LLMs.</b>
</p>

<p>
We demonstrate that: <i>i)</i> MLE can truly be used to steal LLMs, but <i>ii)</i> it suffers from a high complexity of query times.
</p>

<p>
We propose a new RL-based method for this task and show its effectiveness and intrinsic watermark resistance.
</p>
</div>
</div>

<div id="outline-container-orgc21c6f6" class="outline-4">
<h4 id="orgc21c6f6"><span class="section-number-4">1.3.2.</span> Prompt Leakage in LLMs</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models [Preprint]</li>
</ul>


<div id="orgc6e75d9" class="figure">
<p><img src="./images/screenshot_20250309_221310.png" alt="screenshot_20250309_221310.png">
</p>
</div>


<div id="orgdfe6775" class="figure">
<p><img src="./images/screenshot_20250309_221323.png" alt="screenshot_20250309_221323.png">
</p>
</div>

<p>
This paper uncovers the threat of <b>prompt leakage</b> on customized prompt-based services, such as OpenAI's GPTs. It aims to answer three questions:
</p>
<ol class="org-ol">
<li>Can LLM's alignments defend against prompt extraction attacks?</li>
<li>How do LLMs leak their prompts?</li>
<li>Which factors of prompts and LLMs lead to such leakage?</li>
</ol>


<p>
We provide a comprehensive and systemic evaluation to answer question 1 and 3, and propose two hypotheses with experimental validation for question 2. We also propose several easy-to-adopt defending strategies based on our discovery.
</p>

<p>
Click <a href="https://arxiv.org/abs/2408.02416">here</a> if you are also interested in this research.
</p>
</div>
</div>

<div id="outline-container-org4528183" class="outline-4">
<h4 id="org4528183"><span class="section-number-4">1.3.3.</span> Private Inference in LLMs</h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li>MERGE: Fast Private Text Generation [AAAI'24]</li>
</ul>


<div id="org620ceea" class="figure">
<p><img src="./images/screenshot_20250309_221412.png" alt="screenshot_20250309_221412.png">
</p>
</div>

<p>
This paper proposes a new privacy-preserving inference framework for current transformer-based generative language models based on Secret Sharing and Multi-party Security Computation (MPC). It is also the <b>first</b> private inference framework specifically designed for NLG models. 10x of speedup is provided via our propose method.
</p>

<p>
If you are curious about how cryptography protects the privacy of user contents and models and how we optimize the inference procedure, click <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29964">here</a> for more details.
</p>
</div>
</div>

<div id="outline-container-orgde7c4e9" class="outline-4">
<h4 id="orgde7c4e9"><span class="section-number-4">1.3.4.</span> Mining the Sources of AI Alignments</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li><p>
Exploring Intrinsic Alignments within Text Corpus. [AAAI'25]
</p>


<div id="orgfc12820" class="figure">
<p><img src="./images/screenshot_20250309_222112.png" alt="screenshot_20250309_222112.png"> 
</p>
</div>

<p>
This paper explores the possibility of utilizing the intrinsic signal within raw dialogue texts as the feedback signal for current LLMs. Under a prior distribution of text corpus, we propose a method to sample potentially safer responses without human annotation information.
</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org0d931cc" class="outline-2">
<h2 id="org0d931cc"><span class="section-number-2">2.</span> Experiences</h2>
<div class="outline-text-2" id="text-2">
<ol class="org-ol">
<li>2016.09-2020.06: Bachelor Degree, in Northeastern University, on <i>cybernetics (Control Theory)</i>;</li>
<li>2020.09-2023.06: Master Degree, in the iMiss Group of Xi'an Jiaotong University, on <i>software engineer</i> and research for <i>Conversational AI</i> and <i>NLP Security</i>;</li>
<li>2023.11-now: PhD Student, in the The Hong Kong Polytechnic University in Hong Kong. Research of interests: <i>AI safety, privacy and security</i> and <i>Natural Language Processing</i>.</li>
</ol>
</div>
</div>
<div id="outline-container-orgd1cc1b6" class="outline-2">
<h2 id="orgd1cc1b6"><span class="section-number-2">3.</span> Publications</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org6b9883b" class="outline-3">
<h3 id="org6b9883b"><span class="section-number-3">3.1.</span> As the First Author</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>A Work on LoRA (Under Review)</li>
<li>A Benchmark (Under Review)</li>
<li><i>Exploring Intrinsic Alignments within Text Corpus.</i> <b>Zi Liang</b>, Pinghui Wang, Ruofei Zhang, Haibo Hu, &#x2026; - &lt;AAAI'25, Oral&gt;[<a href="https://github.com/liangzid/TEMP">Code</a>]</li>
<li><i>"Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation.</i> <b>Zi Liang</b>, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu - &lt;Preprint&gt; [<a href="https://arxiv.org/abs/2409.02718">Paper</a>] [<a href="https://github.com/liangzid/LoRD-MEA">Code</a>]</li>
<li><i>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models.</i> <b>Zi Liang</b>, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li - &lt;Preprint&gt; [<a href="https://arxiv.org/abs/2408.02416">Paper</a>][<a href="https://github.com/liangzid/PromptExtractionEval">Code</a>]</li>
<li><i>MERGE: Fast Private Text Generation.</i>  <b>Zi Liang</b>, P Wang, R Zhang, Nuo Xu, Shuo Zhang, Lifeng Xing… - &lt;AAAI'24&gt; [<a href="https://arxiv.org/abs/2305.15769">Paper</a>] [<a href="https://github.com/liangzid/MERGE">Code</a>]</li>
</ul>
</div>
</div>
<div id="outline-container-orgcc1054c" class="outline-3">
<h3 id="orgcc1054c"><span class="section-number-3">3.2.</span> As the main contributor</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>Cross-Modal 3D Representation with Multi-View Images and Point Clouds. Ziyang Zhou, Pinghui Wang, <b>Zi Liang</b>, Haitao Bai, Ruofei Zhang. - &lt;CVPR'25&gt;</li>
<li><i>PAIR: Pre-denosing Augmented Image Retrieval Model for Defending Adversarial Patches.</i> Ziyang Zhou, Pinghui Wang, <b>Zi Liang</b>, Rruofei Zhang, Haitao Bai - &lt;MM'24&gt;</li>
<li><i>How Vital is the Jurisprudential Relevance: Law Article Intervened Legal Case Retrieval and Matching.</i> Nuo Xu Pinghui Wang, <b>Zi Liang</b>, Junzhou Zhao, Xiaohong Guan &lt;Preprint&gt;</li>
</ul>
</div>
</div>
<div id="outline-container-org0044d3f" class="outline-3">
<h3 id="org0044d3f"><span class="section-number-3">3.3.</span> As one of co-authors</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li><i>TSFool: Crafting Highly-Imperceptible Adversarial Time Series through Multi-Objective Attack.</i> Yanyun Wang, Dehui Du, Haibo Hu,  <b>Zi Liang</b>, Yuanhao Liu - &lt;ECAI'24&gt;</li>
<li><i>Multi-action dialog policy learning from logged user feedback.</i> Shuo Zhang, Junzhou Zhao, Pinghui Wang, T Wang,  <b>Zi Liang</b>, Jing Tao… - &lt;AAAI'23&gt;</li>
</ul>



<p>
Note: This page is only about my research, for myself you can see the <a href="https://liangzid.github.io/about.html">personal page</a>. 
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr class="Solid"> <div class="info"> <span class="author">Author: liangzid (<a href="mailto:2273067585@qq.com">2273067585@qq.com</a>)</span> <span class="date">Create Date: Fri Sep 10 14:23:46 2021</span> <span class="date">Last modified: 2025-03-09 Sun 22:36</span> <span>Creator: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.2 (<a href="https://orgmode.org">Org</a> mode 9.6.28)</span> </div>
</div>
</body>
</html>
